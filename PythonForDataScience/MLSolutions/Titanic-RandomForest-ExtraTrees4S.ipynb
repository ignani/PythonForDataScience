{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b4d3be2-c62c-4cf7-9a70-704a97a565b9",
    "_uuid": "70ab8e37a3a154afff4eed7ac0fc8398b9c7a49c"
   },
   "source": [
    "# Titanic - Random Forest (Extra Trees) Trial - v3 [0.79904]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "443f0028-70c6-4676-a8f1-eba2ee94c3d1",
    "_uuid": "69a763fcc5660dcb76e29fc0633cf9102fa0c75f"
   },
   "source": [
    "Here, an Extremely Randomized Trees (ExtraTrees) model is used to predict survival on the Titanic.  A new variable is created to hold (essentially) the number of people sharing a ticket, which is similar to cabin share.  Tuning of some ExtraTrees parameters is performed.  The model uses binary encodings for categorial variables.\n",
    "\n",
    "This kernel is similar to my previous kernel, but it assumes that the survival of mother/nannies and children traveling together are related.\n",
    "\n",
    "The number of samples considered at a split seems to be an important parameter in improving model accuracy.  The ticket share variable does not provide much additional information beyond existing fields.  The final prediction is based on several training runs, as the random state of the system affects outcomes. Binary encoding of categorical variables may slightly improve predictions, but reduces interpretability.\n",
    "\n",
    "### Contents\n",
    "1.  Data Import and Setup\n",
    "2.  Data Exploration, Cleaning, and Preparation\n",
    "  1.  Name - extract title \n",
    "  2. Cabin - Extract deck level and examine missing decks\n",
    "  3.  Ticket/cabin sharing\n",
    "  4. Missing data - Embarcation point\n",
    "  5. Fare - missing / zero values\n",
    "  6.  Age - Missing data \n",
    "  7. Nanny/Mother and Child Survival\n",
    "3.  Gathering Analysis Fields\n",
    "  1.  Binary encoding of categorical variables\n",
    "  2.  Function to process and clean the data set\n",
    "4.  ExtraTrees Model - Testing and Optimization\n",
    "  1.  Preliminary ExtraTrees run and feature importances\n",
    "  2.  Random effects on model predictions - train/test split and random state\n",
    "  3.  Evaluation of the \"ticket share\" field.\n",
    "  4.  Binary vs. one-hot encoding of categorical fields\n",
    "  5.  Model tuning - tree count\n",
    "  6.  Model tuning- number of samples used in splits\n",
    "  7.  Model tuning - max feature counts\n",
    "  8.  Model tuning - max number of leaf nodes\n",
    "  9.  Model tuning - tree count with larger split samples\n",
    "5.  Predictions and Submission\n",
    "  1.  Function - predictions from tuned model\n",
    "  2.  Ensembling - repeating models and combining results\n",
    "  3.  Final training and submission data\n",
    "6.  Discussion\n",
    "7.  References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78c69c29-3075-42c8-bc3b-9e58cdd1d5bf",
    "_uuid": "8d4fce6117829401e5ad6bea8d4ee1f73ec55bd4"
   },
   "source": [
    "## 1.  Data Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7eb32e95-171d-4f28-97fa-095b79cf386e",
    "_uuid": "5a1bb54b85d723c44fb4706cee062cab6a62b903"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a7ceb886-4298-4489-9da6-9c1aeef1d03d",
    "_uuid": "c4e7da62827c22cf046ac2b4c0909db3ed4acc08",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sklearn as sk\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ce26c6d-5ce2-4d0b-a878-137ebd0bd3b7",
    "_uuid": "38cff10820edcc6b62d64e082dd3d57d8dce0a35"
   },
   "source": [
    "### Import training data\n",
    "\n",
    "Create the total family size (Parch + SibSp) here.  Enforce some data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "beee0328-6cc1-4bdf-800d-ee04d820bd8d",
    "_uuid": "d91d576280ea0fb1d8e3712a5ec5586ff6a9d879"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/train.csv' does not exist",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7a70c9344e29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                          na_values={'Cabin':'', 'Ticket':'', \n\u001b[0;32m     10\u001b[0m                                     'Embarked':'', 'Sex':''}, \n\u001b[1;32m---> 11\u001b[1;33m                          keep_default_na=False)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Set the index and create a family size field. Make a single a family size of 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../input/train.csv' does not exist"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Import the test and training data.\n",
    "#\n",
    "\n",
    "train_data = pd.read_csv(\"../input/train.csv\",\n",
    "                        dtype={'Cabin':np.object, 'Ticket':np.object, 'Age':np.float,\n",
    "                              'SibSP':np.int, 'Parch':np.int, 'Fare':np.float,\n",
    "                              'Embarked':np.object, 'Sex': np.object}, \n",
    "                         na_values={'Cabin':'', 'Ticket':'', \n",
    "                                    'Embarked':'', 'Sex':''}, \n",
    "                         keep_default_na=False)\n",
    "\n",
    "# Set the index and create a family size field. Make a single a family size of 1\n",
    "train_data.set_index('PassengerId', inplace=True)\n",
    "\n",
    "test_data = pd.read_csv(\"../input/test.csv\",\n",
    "                        dtype={'Cabin':np.object, 'Ticket':np.object, 'Age':np.float,\n",
    "                              'SibSP':np.int, 'Parch':np.int, 'Fare':np.float,\n",
    "                              'Embarked':np.object, 'Sex': np.object}, \n",
    "                         na_values={'Cabin':'', 'Ticket':'', \n",
    "                                    'Embarked':'', 'Sex':''}, \n",
    "                         keep_default_na=False)\n",
    "\n",
    "# Set the index and create a family size field. Make a single a family size of 1\n",
    "test_data.set_index('PassengerId', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56a9b4e5-3bea-4f75-8f19-73c19e1cb394",
    "_uuid": "771928688cd70745c8a704e8e9914b22dc34534f"
   },
   "source": [
    "Create a copy of the data for exploration. Include the training and test data.  This data set will be used to examine missing data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2db62120-bcea-43e8-a0fe-dfaf99b7dc19",
    "_uuid": "6dd1394f8cb76ed6bf34f90ee96de8bdca00989a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9ca23fa882fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'family_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SibSp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Parch'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df_analysis = pd.concat([train_data, test_data], axis=0)\n",
    "df_analysis['family_size'] = df_analysis['SibSp'] + df_analysis['Parch'] + 1\n",
    "\n",
    "df_analysis.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "617abdfe-b08c-4389-b58f-2fbb78efec15",
    "_uuid": "3ea8ff928ffcddfe9d973c3e7106f50c69c498c7"
   },
   "source": [
    "## 2.  Data exploration, cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8b19846e-9d44-42ec-b28c-ee480bd8b4e4",
    "_uuid": "1c1851c9e63c75085d804c6c1b0269c67d80783f"
   },
   "source": [
    "### 2A.  Name - extract title.\n",
    "I want to get the title (Mr/Mrs/Miss etc.) I will group uncommon titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e1ae41f9-fb3a-4d62-beb4-9e20010ef532",
    "_uuid": "3000f1979b56c40984bcd5ce3b30407d8d49350e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_title_str(name_str):\n",
    "    \"\"\"Extract a title from a name string\"\"\"\n",
    "    mobj = re.search(\", (.*?)[\\.| ]\", name_str)\n",
    "    mobj_str = (mobj.group()\n",
    "                .strip(' ,.').\n",
    "                lower())\n",
    "    return(mobj_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67bd1ea6-fbe1-4833-958e-d7caae68796f",
    "_uuid": "f5ab8fc01be11ae65eef21bf6950fcdea6038fce"
   },
   "source": [
    "Examine title frequencies.  Grouping the \"other\" category may make sense given low counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "aa7312c9-17cd-4e64-85f3-fd1ae2fd4bf9",
    "_uuid": "5e4a5a6f70424d7284a57c8aa8ff444574fa39c5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3292ccd550a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitle_ser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_title_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtitle_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtitle_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "title_ser = df_analysis['Name'].apply(extract_title_str)\n",
    "title_ser.name ='title'\n",
    "title_ser.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad8bfcb3-1f6b-47ba-a55b-d49371256628",
    "_uuid": "7260d61cf20f51e871310954e9484ffaf85372d4"
   },
   "source": [
    "Get another series of title groups - keep the major categories (mr/mrs/miss/master) and group others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b3d2d3ac-e1c8-4c71-9e01-26be5a682250",
    "_uuid": "1ff9173fe99dcb3e15d00a834d91fb9f3284277a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_ser' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4c05e8cf870a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitle_grp_ser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mrs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'miss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'master'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'other'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtitle_grp_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'title_grp'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtitle_grp_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title_ser' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "title_grp_ser = title_ser.apply(lambda x: x if x in ['mr', 'mrs', 'miss', 'master'] else 'other')\n",
    "title_grp_ser.name ='title_grp'\n",
    "title_grp_ser.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ab1c3da4-830e-415a-b02b-faede5190b6d",
    "_uuid": "fa242b45648450a96e8c14f5f1fcb02b2f1a1a96"
   },
   "source": [
    "Append the title and title group and plot to see how title affects survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "6e8cae37-996b-421c-ae45-a5d0dad5d305",
    "_uuid": "03ec26c464f9a7353a94d9e561872811f0740cda"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-61c09303fee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_ser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_grp_ser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magg_df_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'title_grp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magg_df_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_df_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Survival probability by title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df_analysis = pd.concat([df_analysis, title_ser, title_grp_ser], axis=1)\n",
    "\n",
    "agg_df_title = df_analysis.groupby('title_grp').agg({'Survived':['mean','sem']})\n",
    "agg_df_title['Survived', 'mean'].plot(kind='bar', yerr=agg_df_title['Survived', 'sem'], alpha = 0.5, error_kw=dict(ecolor='k'));\n",
    "plt.gcf().suptitle('Survival probability by title');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "566ef36f-0e35-4477-a37a-8b04c64a75c3",
    "_uuid": "2a8d2b56d8cfc3bc1a66ba7cf64e727948198e9d"
   },
   "source": [
    "### 2B. Cabin - Extract deck level and examine missing decks\n",
    "The deck level can be extracted from the Cabin field.  I will get the deck and examine unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9a2998fa-29ef-49ef-abf5-8d72cad2e577",
    "_uuid": "c9140b8fbfee790251af1f2f0112eec71d4773f8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-45c392f72bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcabin_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cabin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_deck_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Examine the cabin/deck variable.  Many are missing\n",
    "#\n",
    "\n",
    "def extract_deck_str(cabin_str):\n",
    "    \"\"\"Extract a deck letter from a cabin string\"\"\"\n",
    "    return(cabin_str[0:1].upper())\n",
    "\n",
    "df_analysis['Cabin'].apply(extract_deck_str).value_counts().sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a485939-30a2-4521-8256-4aa435b1d80c",
    "_uuid": "103d613d0ea5811f5141fee660119414d4bf8f0a"
   },
   "source": [
    "Add a column to the data frame, 'deck_mod', which contains the original deck with T/G grouped into 'TG'. How is survival affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "5dceb008-7505-4de9-b96a-cf34b7c03b2d",
    "_uuid": "62db166135e41d3230672d14784f10ace7dd69d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b58db8636687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeck_ser\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cabin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_deck_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m deck_ser_mod = deck_ser.apply(lambda x: x if x in ['A','B','C','D','E','F'] \n\u001b[0;32m      3\u001b[0m                               else 'TG' if x in ['T', 'G'] else 'unk')\n\u001b[0;32m      4\u001b[0m \u001b[0mdeck_ser_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'deck_mod'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "deck_ser= (df_analysis['Cabin']).apply(extract_deck_str)\n",
    "deck_ser_mod = deck_ser.apply(lambda x: x if x in ['A','B','C','D','E','F'] \n",
    "                              else 'TG' if x in ['T', 'G'] else 'unk')\n",
    "deck_ser_mod.name = 'deck_mod'\n",
    "\n",
    "df_analysis = pd.concat([df_analysis, deck_ser_mod], axis=1)\n",
    "df_analysis.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "c97af69c-3bca-435e-ad1a-6e3d1b9d0f17",
    "_uuid": "19c019ef9db5f507725873ebd1a3e737f69e73c7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93235df31d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magg_df_deck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'deck_mod'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m agg_df_deck['Survived', 'mean'].plot(kind='bar', \n\u001b[0;32m      3\u001b[0m                                      \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_df_deck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                      alpha = 0.5, error_kw=dict(ecolor='k'));\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Survival probability by deck'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agg_df_deck = df_analysis.dropna().groupby('deck_mod').agg({'Survived':['mean','sem']})\n",
    "agg_df_deck['Survived', 'mean'].plot(kind='bar', \n",
    "                                     yerr=agg_df_deck['Survived', 'sem'], \n",
    "                                     alpha = 0.5, error_kw=dict(ecolor='k'));\n",
    "plt.gcf().suptitle('Survival probability by deck');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8612aa95-bb0d-46bc-88fb-6e4cce91d090",
    "_uuid": "9e3b8c0639800bfece89ddf2eb6247dcced6c08d"
   },
   "source": [
    "It appears that missing deck cases may have a lower survival probability than cases with deck strings.  Look at fares by deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "f07023d5-7b7e-4d0f-90d5-d5a3d344c0cc",
    "_uuid": "074b8e532bd89e8ca510cdce39c9a4e445aa6c31"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-17e7392ebf60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magg_df_deck2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'deck_mod'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m agg_df_deck2['Fare', 'mean'].plot(kind='bar', \n\u001b[0;32m      3\u001b[0m                                      yerr=agg_df_deck2['Fare', 'sem'], alpha = 0.5, error_kw=dict(ecolor='k'));\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean fare by deck'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agg_df_deck2 = df_analysis.groupby('deck_mod').agg({'Fare':['mean','sem']})\n",
    "agg_df_deck2['Fare', 'mean'].plot(kind='bar', \n",
    "                                     yerr=agg_df_deck2['Fare', 'sem'], alpha = 0.5, error_kw=dict(ecolor='k'));\n",
    "plt.gcf().suptitle('Mean fare by deck');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3690ddcd-0302-4f6b-87f1-dcf459f2c42a",
    "_uuid": "18a38e1d402c229d405554d56a1f337a7c42c18b",
    "collapsed": true
   },
   "source": [
    "Looking at the above information, it seems that the people with missing deck information may be different than those with deck information.  Therefore, instead of trying to infer a deck, I will treat them a separate group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "acec8df2-4f46-4d84-8116-afbf3eeebebb",
    "_uuid": "f3d4351c90f7712a005f0100adf05da946a946df"
   },
   "source": [
    "### 2C. Ticket/cabin sharing\n",
    "The ticket number appears to be filled in for all individuals.  It is possible to find out who shares a ticket.  Sometimes, family members seem to share a ticket and sometimes not.  \n",
    "\n",
    "I am assuming that people who share a ticket share a cabin.  I will roughly test this by looking at the # of people who share a ticket vs the # sharing a cabin.  This grouping uses the entire data set (test + training), because some people who share a cabin may be in different data sets.\n",
    "\n",
    "Ticket numbers are cleaned prior to grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "646e45bf-971c-4a5e-b625-f1ab6f35e595",
    "_uuid": "111fe48d4a97ed2c461b35e429f4d199773e0f50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define some functions for calculating ticket and cabin shares\n",
    "#\n",
    "\n",
    "def cabin_share_series(cabin_ser):\n",
    "    \"\"\"Returns a series containing the # of people sharing each\n",
    "    passenger's cabin, when this number can be determined\"\"\"\n",
    "    \n",
    "    # Get the passenger counts per ticket\n",
    "    agg_ser = cabin_ser[cabin_ser != ''].value_counts()\n",
    "    agg_ser.name ='cabin_share'\n",
    "    \n",
    "    df_ret = pd.merge(cabin_ser.to_frame(), agg_ser.to_frame(), \n",
    "                      how='left', left_on='Cabin', right_index=True,\n",
    "                      indicator=False)\n",
    "    \n",
    "    return(df_ret['cabin_share'])\n",
    "\n",
    "\n",
    "def clean_ticket(ticket_str):\n",
    "    \"\"\"Clean up the ticket string by replacing punctuation, converting\n",
    "    to upper case, and removing whitespace\"\"\"\n",
    "    ticket_str_clean = re.sub(r'[^\\w]', '', ticket_str.strip())\n",
    "    return (ticket_str_clean)\n",
    "\n",
    "def ticket_count_frame(ticket_ser):\n",
    "    \"\"\"Return a data frame containing 2 series:\n",
    "        ticket_clean: cleaned-up version of ticket string\n",
    "        ticket_share_count: number of people with a certain ticket\"\"\"\n",
    "    \n",
    "    ticket_clean_ser = ticket_ser.apply(clean_ticket)\n",
    "    ticket_clean_ser.name = 'ticket_clean'\n",
    "    \n",
    "    # Get the passenger counts per ticket\n",
    "    agg_ser = ticket_clean_ser.value_counts()\n",
    "    agg_ser.name ='ticket_share'\n",
    "\n",
    "    df_ret = pd.merge(ticket_clean_ser.to_frame(), agg_ser.to_frame(), \n",
    "                      how='left', left_on='ticket_clean', right_index=True,\n",
    "                      indicator=False)\n",
    "    \n",
    "    return(df_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3e65354-f022-4c0b-bca0-e4738f6682d3",
    "_uuid": "af68dfa932c91d9020253319b8fff44fd0cea152"
   },
   "source": [
    "Look at whether the cabin and ticket shares are similar, in which case we can use the ticket\n",
    "share as an estimate of cabin occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "98166374-8ea0-4dd1-84c1-642a541683c9",
    "_uuid": "f7463e81063792dbdcec4fe5c06159cb6686427a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c93ca6740e59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m share_df = pd.concat([cabin_share_series(df_analysis['Cabin']),\n\u001b[0m\u001b[0;32m      7\u001b[0m                          ticket_count_frame(df_analysis['Ticket'])], axis=1)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Get cabin information from test and training data.\n",
    "#\n",
    "\n",
    "\n",
    "share_df = pd.concat([cabin_share_series(df_analysis['Cabin']),\n",
    "                         ticket_count_frame(df_analysis['Ticket'])], axis=1)\n",
    "\n",
    "\n",
    "#\n",
    "# Is the ticket share a good approximation of cabin share?\n",
    "# Get a grid for ticket vs cabin share, with counts\n",
    "#\n",
    "\n",
    "cross_share_df = pd.crosstab(pd.Categorical(share_df['ticket_share']), \n",
    "                    pd.Categorical(share_df['cabin_share']))\n",
    "\n",
    "cross_share_df.index.names= ['Ticket share']\n",
    "cross_share_df.columns.names=['Cabin share']\n",
    "\n",
    "cross_share_df.head()\n",
    "sns.heatmap(cross_share_df, cmap='RdYlGn_r', linewidths=0.5, annot=True,\n",
    "           cbar_kws={'label': '# of passengers'});\n",
    "plt.gcf().suptitle('Do people with the same ticket # tend to share a cabin?');\n",
    "\n",
    "#\n",
    "# These are not that similar, especially for some tickets\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "d6f97c9b-847e-42e8-b9a2-03abd43ac269",
    "_uuid": "c96ce270dba473069c96eafa1012ad488a20cd83"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b46b100a38d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m df_analysis = pd.merge(left=df_analysis, right=share_df[['ticket_share', \n\u001b[0m\u001b[0;32m      6\u001b[0m                                                          \u001b[1;34m'cabin_share'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                          'ticket_clean']],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Add the ticket share to the frame\n",
    "#\n",
    "\n",
    "df_analysis = pd.merge(left=df_analysis, right=share_df[['ticket_share', \n",
    "                                                         'cabin_share',\n",
    "                                                         'ticket_clean']],\n",
    "                       left_index = True, right_index= True, how='left')\n",
    "df_analysis.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "65ace321-02ca-47b3-8853-b3e9c99a4afd",
    "_uuid": "a5d791653a1fb40b2c0144f08ae0ec8487391c45"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ccee03243102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'null count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticket_share'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticket_share'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('null count: ', df_analysis['ticket_share'].isnull().sum())\n",
    "df_analysis['ticket_share'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d8d6be3e-a668-4e93-a486-0a3129e45548",
    "_uuid": "29a1d47664fd0778c9f733a1f8240153e25f574e"
   },
   "source": [
    "Look at whether the ticket share is similar to family size.  Do non-family share tickets?  Do families always share?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "186aad63-be6d-474f-910d-29861a80b83d",
    "_uuid": "018931f24cf2d2d0d3d97787b7c648f07555e52e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ae1fa219b99a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m family_ticket_share_df = pd.crosstab(pd.Categorical(df_analysis['ticket_share']), \n\u001b[0m\u001b[0;32m      7\u001b[0m                     pd.Categorical(df_analysis['family_size']))\n\u001b[0;32m      8\u001b[0m \u001b[0mfamily_ticket_share_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Ticket share'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Examine the ticket share vs. family size.\n",
    "# Leave off upper left corner of singletons to get better heat\n",
    "#\n",
    "\n",
    "family_ticket_share_df = pd.crosstab(pd.Categorical(df_analysis['ticket_share']), \n",
    "                    pd.Categorical(df_analysis['family_size']))\n",
    "family_ticket_share_df.index.names= ['Ticket share']\n",
    "family_ticket_share_df.columns.names=['Family size']\n",
    "family_ticket_share_df.loc[1,1] = np.nan\n",
    "\n",
    "sns.heatmap(family_ticket_share_df, \n",
    "            cmap='RdYlGn_r', linewidths=0.5, annot=True,\n",
    "                      cbar_kws={'label': '# of passengers'});\n",
    "plt.gcf().suptitle('Are ticket shares and family size related?');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef05b690-8bc4-4c96-80e2-ec2797c2e5e7",
    "_uuid": "6f657162b41635f210a68969fc17082633911a49"
   },
   "source": [
    "It's clear that ticket and family size are correlated.  However, there are a fair number of non-family ticket sharers. This information could be helpful  There are likely also some family members in separate cabins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a35abaaa-370f-4fbe-804f-c453b1d4aff5",
    "_uuid": "23c5cb19f9062f472deaf893c9f6c13aecb7bbf2"
   },
   "source": [
    "Get another heatmap, this time for survival rates by family size and ticket share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "5bfd4042-8f4d-4a55-ad06-99b0d4fa40ea",
    "_uuid": "e408f657c3817d30f9e80763e7b06d905d3ed31c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3f398ea28fff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magg_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'family_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ticket_share'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'std'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0magg_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magg_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdroplevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magg_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Remove very low-population cells and unstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agg_df = df_analysis.groupby(['family_size', 'ticket_share']).agg({'Survived':['mean','std', 'count']})\n",
    "agg_df.columns = agg_df.columns.droplevel()\n",
    "agg_df.head()\n",
    "\n",
    "# Remove very low-population cells and unstack\n",
    "agg_df2 = agg_df[agg_df['count'] >= 3]['mean'].unstack(level=1).T\n",
    "\n",
    "sns.heatmap(agg_df2, cmap='viridis', linewidths=0.5, annot=True,\n",
    "                      cbar_kws={'label': 'Survival Probability'});\n",
    "plt.gcf().suptitle('Survival by family size, ticket share');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1dde9a92-48e3-45bb-bb70-1fbd5201dba6",
    "_uuid": "ee9f229d8a17f2c55c6e441203aed74e57d501d4"
   },
   "source": [
    "There are some interesting off-diagnoal elements here! Particularly for unrelated people sharing a cabin. In addition, large family may be less of a risk if the family does not share 1 ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee656f37-dfd7-4781-bb8f-663264e16c56",
    "_uuid": "535034dbdc867a9a6fada18e45c5df67ba4d23f3"
   },
   "source": [
    "### 2D. Missing data - Embarcation point\n",
    "Infer embarcation point from other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "da95acb6-a766-4ffa-8ff8-0b9b5de11a41",
    "_uuid": "2812c8f4d76f03db8dff099fc56062504b708abd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6dd567c01a2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# View missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Embarked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# View missing values\n",
    "df_analysis['Embarked'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "629ef9e9-a1ef-493d-8459-9b4860c57993",
    "_uuid": "786d0a5bcd08af98a542b011fa49a892fe6252c2"
   },
   "source": [
    "How many are missing embaration points? Very few.  I will set missing values to the most common embarcation point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "c2cf65af-23e0-43b5-83a6-721baf5f6f9a",
    "_uuid": "4e791ae3560d5aec7de66094f5df8c06c45506c1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-916f5e497cc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membarked_ser\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Embarked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0membarked_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'embarked_mod'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membarked_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "embarked_ser= df_analysis['Embarked'].apply(lambda x: x if x != '' else 'S')\n",
    "embarked_ser.name = 'embarked_mod'\n",
    "embarked_ser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "7212ced8-cd4b-4bf4-add9-60197d492a8c",
    "_uuid": "d411b874bf9eb12668a54a7bf036e871fd53df3f",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-44b69b0a1ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add the embarcation point to the frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membarked_ser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Add the embarcation point to the frame\n",
    "df_analysis = pd.concat([df_analysis, embarked_ser], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41e41f58-c7a1-4b19-a65f-dd92cc40b3ec",
    "_uuid": "651de32da8bb410533a4302155560056faa97bae"
   },
   "source": [
    "### 2E. Fare - missing / zero values\n",
    "My training data does not contain missing fares.  However, the test data does contain missing fares.  \n",
    "\n",
    "Also, Some passengers have a $0 fare.  Is this a proxy for missing data?\n",
    "\n",
    "I can't say for sure, but looking at the data, these are all males of working age who share an embarcation point and have a family size 1.  I wonder if they are employees or some other group who have \\$0 fare for a reason.  Therefore, I will not fill in \\$0 fares.  However, missing fares should be handled.  Since this is so rare, I will simply set this to the median fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b009f659-4b62-42b7-ae35-8c638f548e5f",
    "_uuid": "821ad0f50013406591a3c4aca24ce0986764b96c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-57314a119111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training null count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training zero count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test null count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test zero count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('training null count: ', df_analysis['Fare'].isnull().sum())\n",
    "print('training zero count: ', df_analysis[df_analysis['Fare']<= 0]['Fare'].count())\n",
    "\n",
    "print('test null count: ', test_data['Fare'].isnull().sum())\n",
    "print('test zero count: ', test_data[test_data['Fare']<= 0]['Fare'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3a9c931-011b-4860-9183-0644f086344c",
    "_uuid": "14802db15cdea92b0658cc267fca8af9d3e5e5ca"
   },
   "source": [
    "How does fare affect survival?  Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "1777938a-4796-4255-8b11-689170993bae",
    "_uuid": "5f0fbf3da1ecdad2860b319812aef9ffb5e673f8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-43ba1102a8f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Truncate high outliers in plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m140\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "sns.boxplot(x='Survived', y='Fare', data = df_analysis);\n",
    "# Truncate high outliers in plot\n",
    "plt.ylim([0,140]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84e870d9-8383-401c-83bd-5ccf42c7ea83",
    "_uuid": "362e8b228974421f09f52207dcd3b4c741da3432"
   },
   "source": [
    "### 2F.  Age - Missing data \n",
    "A large number of people have missing (NaN) ages.  Infer age from other information for these people.  All ages are floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "2cfc9211-2ae7-4a51-a3ba-dc940f112e20",
    "_uuid": "516e2d3debf8173c81816a808df3563e1fa6c665"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0c415a1b6abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'null count: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('null count: ', df_analysis['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "a04b1a10-104e-498b-833f-694b4f90ad2c",
    "_uuid": "6b0faa076b23004fd313f1422e51ec3423c14ef0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f1e4cbf29661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's look at an age histogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# There are many infants aboard.  I wondered if these were legitimate.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Let's look at an age histogram\n",
    "\n",
    "ax = sns.distplot(df_analysis['Age'].dropna())\n",
    "\n",
    "# There are many infants aboard.  I wondered if these were legitimate.\n",
    "# Looking at titles and ages they seem to be.\n",
    "# There are no 0.0 ages, for example.  Titles tend to be \"miss\" or \n",
    "# \"master\".  Therefore, this peak seems real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6530a52-0f31-420f-9838-ad679adca2db",
    "_uuid": "bc63861089a0bde7775f7b644bf537394fb67a23"
   },
   "source": [
    "I want to fill in missing values of age using other information in the data set.  To this end, I will plot some bar charts to visualize how age depends on various data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "69660312-1bf8-4b8c-b7e1-b5dd8ca9ee79",
    "_uuid": "ac370683ccafc1ad921135612d7c41539a421638"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6faf007347dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                     'Sex', 'Parch', 'embarked_mod', 'deck_mod']\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupby_field_vec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mind_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'42'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroupby_field_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-6faf007347dd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m                     'Sex', 'Parch', 'embarked_mod', 'deck_mod']\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sem'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupby_field_vec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mind_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'42'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroupby_field_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# vector of interesting features\n",
    "groupby_field_vec= ['SibSp', 'family_size', 'Pclass', 'title_grp', \n",
    "                    'Sex', 'Parch', 'embarked_mod', 'deck_mod']\n",
    "\n",
    "df_list = [df_analysis.groupby(g).agg({'Age':['mean','sem']}) for g in groupby_field_vec]\n",
    "ind_list = [int('42' + str(x)) for x in range(1,len(groupby_field_vec)+1)]\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    df.columns = df.columns.droplevel()\n",
    "    df['mean'].plot(kind='bar', ax=plt.subplot(ind_list[i]),\n",
    "                   yerr = df['sem'], figsize=(9,9));\n",
    "\n",
    "plt.suptitle('Age by various factors')\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=0, right=None, top=0.9, wspace=0.3, hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbe5ad04-344d-4da0-8a89-4f9d7fc8ec41",
    "_uuid": "acce1dcafb2848a1b69d8adb79d7f6db4e460a42"
   },
   "source": [
    "Based on the above, I will set the passenger age to the median for  the combination of the following fields:\n",
    "    * title_grp\n",
    "    * Pclass\n",
    "    * family_size (thresholded <=7)\n",
    "    * Sex\n",
    "    * Parch (thresholded <=7)\n",
    "    * deck_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "18ce1621-4f1c-48e0-adf4-a8adfe22bdd2",
    "_uuid": "4200d6ab839820b3a55e3356da1dc2b290fb2385"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7855e60a9cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'age_grp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Look at survival by age\n",
    "#\n",
    "\n",
    "df2 = df_analysis.dropna().copy()\n",
    "df2['age_grp'] = df2['Age'].apply(lambda x: 0 if x < 1 else 1+ int(x/10))\n",
    "\n",
    "agg_df_age = df2.groupby('age_grp').agg({'Survived':['mean','sem']})\n",
    "agg_df_age['Survived', 'mean'].plot(kind='bar', yerr=agg_df_age['Survived', 'sem'], alpha = 0.5, error_kw=dict(ecolor='k'));\n",
    "plt.gcf().suptitle('Survival probability by age');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "905d76af-9bbd-49b2-9b9c-ff318e529987",
    "_uuid": "de390b4c03ff6531941e6279746dedf84026a1dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Function to fill in missing ages\n",
    "#\n",
    "\n",
    "def age_filled_ser(age_df):\n",
    "    \"\"\"Get a series of inferred/actual passenger ages.  If the age is not NaN,\n",
    "    it is returned.  Otherwise, the age is inferred using the information in age_df.\n",
    "       The data frame input must contain the following fields:\n",
    "        Age\n",
    "        title_grp\n",
    "        Pclass\n",
    "        family_size\n",
    "        Sex\n",
    "        Parch\n",
    "        deck_mod\n",
    "    The mean ages for the above fields (except Age) will be used to fill in \n",
    "    missing data. Some values are thresholded.\n",
    "       It's possible for a passenger to have a unique combination of items.\n",
    "    In that case, we do backup fills, removing variables in reverse order\n",
    "    from the list above (e.g. remove deck, then Sex)\n",
    "       Assume an appropriate index in the passed-in frame.  \n",
    "    The output series will be named 'age_mod'\"\"\"\n",
    "    \n",
    "    field_list= ['title_grp', 'Pclass', 'family_size', \n",
    "                       'Sex', 'Parch', 'deck_mod']\n",
    "    \n",
    "    fill_df = age_df[['Age'] + field_list].copy()\n",
    "    \n",
    "    # Threshold some values\n",
    "    fill_df['family_size'] = (fill_df['family_size']\n",
    "                              .apply(lambda x: x if (x <=7) else 7))\n",
    "    fill_df['Parch'] = (fill_df['Parch']\n",
    "                              .apply(lambda x: x if (x <=7) else 7))\n",
    "\n",
    "    \n",
    "    predictor_list = [field_list[0:x] for x in range(len(field_list), 0, -1)]\n",
    "    fill_df['age_mod'] = fill_df['Age']\n",
    "    \n",
    "    # Fill NAs using descending numbers of predictors\n",
    "    for predictors in predictor_list:\n",
    "        fill_df['age_mod'] = (fill_df.groupby(predictors)['age_mod']\n",
    "                          .transform(lambda x: x.fillna(x.mean())))\n",
    "    \n",
    "    return(fill_df['age_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "f5d23c94-d69d-4689-b01c-6bb7fe3733b3",
    "_uuid": "94e9c362e69439c99c8bd6eb26acc5d054124817"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1cdc136557da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mage_mod_ser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mage_filled_ser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'null count:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_mod_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_mod_ser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "age_mod_ser = age_filled_ser(df_analysis)\n",
    "print('null count:', age_mod_ser.isnull().sum())\n",
    "\n",
    "df_analysis = pd.concat([df_analysis, age_mod_ser], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6b96d8b-e3cf-4bd2-92ec-e7e6da6282f8",
    "_uuid": "841bec5cad40abf495976057a56c5441e4a4ddd6"
   },
   "source": [
    "### 2G.  Nanny/Mother and Child Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6ecc88e6-0a42-4fdc-8882-013c44bcaf24",
    "_uuid": "a4b02a450957f85595dfa1a57886d982f3ecb330"
   },
   "source": [
    "Here, I will look at the survival status for the youngest child per ticket, and see how that correlates to the survival of a mother/nanny.  A mother must have Parch > 0, but a nanny must have Parch=0.  In either case, require female sex and age 18+.   A child may have 1 mother or 1 nanny but not both.  (I am defining children as people under 15 for this purpose).\n",
    "\n",
    "I will also look at how survival of other children on a ticket relates to survival of the mother/nanny and youngest child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "9ab85411-0907-4bb2-86a2-81b9cb559d59",
    "_uuid": "825d58e912580c36c106322388fddfd29f329528",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get mother/nanny and child survival statuses\n",
    "#\n",
    "\n",
    "def mother_child_nanny_ticket_frame(ticket_df):\n",
    "    \"\"\"Returns 2 data frames:\n",
    "       1.  child_frame:  A frame consisting of all children, with the child's survival status,\n",
    "           the mother/nanny's survival status, the survival status of the youngest child, \n",
    "           a youngest child flag, and a nanny flag.\n",
    "       2.  mother_nanny_df: A frame consisting of all mother/nannies, with the survival status,\n",
    "           the youngest child's survival status, and a nanny flag.\n",
    "    Data frames are joined by clean ticket number.  \"\"\"\n",
    "    \n",
    "    # Find tickets with children and add a \"nanny flag\"\n",
    "    child_tickets_sort = (ticket_df.copy()[ticket_df['age_mod'] < 15]\n",
    "                          .sort_values(['ticket_clean', 'age_mod', 'Survived', 'Parch'],\n",
    "                                      ascending=[True, True, False, False]))\n",
    "\n",
    "    child_tickets_sort['nanny_flag'] = (child_tickets_sort['Parch']\n",
    "                                        .apply(lambda x: 1 if x ==0  else 0))\n",
    "    \n",
    "    # Get the youngest child's survival status and flag\n",
    "    child_tickets_grp = child_tickets_sort.groupby('ticket_clean')\n",
    "    youngest_child_df = (child_tickets_grp\n",
    "                         .head(1)\n",
    "                         .rename(columns={'Survived': 'survived_youngest'}))[['ticket_clean', \n",
    "                                                                              'nanny_flag', \n",
    "                                                                              'survived_youngest']]\n",
    "    # Get the mother/nannny information\n",
    "    ticket_df_summary = ticket_df.reset_index()\n",
    "    merge_df = pd.merge(ticket_df_summary, youngest_child_df,\n",
    "                        how='right', left_on='ticket_clean', \n",
    "                        right_on='ticket_clean')\n",
    "    mother_nanny_merge_df = merge_df[(merge_df['Sex'] == 'female') & (merge_df['age_mod'] >= 18) &\n",
    "                     ((merge_df['nanny_flag'] == 1) | (merge_df['Parch'] > 0))]\n",
    "    mother_nanny_df = (mother_nanny_merge_df\n",
    "                       .sort_values(['ticket_clean', 'age_mod'])\n",
    "                       .groupby('ticket_clean')\n",
    "                       .head(1))[['Survived', 'ticket_clean',\n",
    "                                  'nanny_flag','survived_youngest', 'PassengerId']]\n",
    "    mother_nanny_df.set_index('PassengerId', inplace=True)\n",
    "    \n",
    "    # Get information for all children - include youngest survival and mother's survival\n",
    "\n",
    "    child_tickets_summary = child_tickets_sort.reset_index()[['PassengerId', 'Survived', 'ticket_clean']]\n",
    "    child_merge_df = pd.merge(child_tickets_summary,\n",
    "                              youngest_child_df[['ticket_clean', 'survived_youngest']],\n",
    "                              how='left', left_on='ticket_clean', right_on='ticket_clean')\n",
    "    mother_nanny_summary = (mother_nanny_df\n",
    "                             .rename(columns={'Survived': 'survived_mother_nanny'}))[['ticket_clean',\n",
    "                                                                                     'nanny_flag', \n",
    "                                                                                     'survived_mother_nanny']]\n",
    "    child_merge_df = pd.merge(child_merge_df, mother_nanny_summary,\n",
    "                              how='left', left_on='ticket_clean', right_on='ticket_clean')\n",
    "    \n",
    "    # Add the youngest child flag\n",
    "    youngest_id_summary = (youngest_child_df.reset_index())[['PassengerId']]\n",
    "    child_merge_df = pd.merge(child_merge_df, youngest_id_summary,\n",
    "                              how='left', left_on='PassengerId', right_on='PassengerId',\n",
    "                             indicator = True)\n",
    "    child_merge_df['youngest_flag'] = child_merge_df['_merge'].apply(lambda x: 1 if x == 'both' else 0)\n",
    "    child_merge_df.set_index('PassengerId', inplace=True)\n",
    "    child_final_df = child_merge_df[['Survived','ticket_clean', 'youngest_flag', 'nanny_flag', 'survived_youngest',\n",
    "                                    'survived_mother_nanny']]\n",
    "    \n",
    "    return(mother_nanny_df, child_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "d565dbee-6626-4c86-9a43-eaa586bd8009",
    "_uuid": "e009ba298aefce04f28e8be7a3401238e7f5d5b6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c4b94c365a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m (mother_nanny_df, child_df) = mother_child_nanny_ticket_frame(df_analysis[['age_mod', 'ticket_clean', \n\u001b[0m\u001b[0;32m      2\u001b[0m                                                                            'Survived', 'Parch', 'Sex']])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "(mother_nanny_df, child_df) = mother_child_nanny_ticket_frame(df_analysis[['age_mod', 'ticket_clean', \n",
    "                                                                           'Survived', 'Parch', 'Sex']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fad785a4-ba41-4f84-8c60-473a54fcbc8f",
    "_uuid": "3828f31abb49e262befaebcd9e905b6c2506b12a"
   },
   "source": [
    "How is mother/nanny survival related to youngest child survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "a5c43a07-0ec4-4cfa-8043-ac44ac67294d",
    "_uuid": "349474c1039107bbcfcb94621ae1ce7cddae85aa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mother_nanny_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-25cdaa9d0ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmother_nanny_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmother_nanny_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'survived_youngest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mother_nanny_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pd.crosstab(mother_nanny_df['Survived'], mother_nanny_df['survived_youngest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38849bbf-8acf-452e-81f0-5c74e1e0f751",
    "_uuid": "f7e0c09b2d1c11e4785ba953edbc39cfa1ca069d"
   },
   "source": [
    "These agree nearly all the time!\n",
    "\n",
    "For non-youngest children, how does survival relate to the survival of the mother/nanny or youngest child?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "febcda39-097c-45c1-8d56-d1701c70499b",
    "_uuid": "0293cbd13be393de742eec39911530e6156fd939"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'child_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e12eeddbc350>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchild_older_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchild_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchild_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'youngest_flag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_older_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_older_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'survived_mother_nanny'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'child_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "child_older_df = child_df[child_df['youngest_flag'] == 0]\n",
    "pd.crosstab(child_older_df['Survived'], child_older_df['survived_mother_nanny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "2071caa7-a3bd-4f0d-9681-ac7acea47132",
    "_uuid": "fa617c0f2e9a1f7350a22942262f09e63b1153fa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'child_older_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0065fca9a6d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_older_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_older_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'survived_youngest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'child_older_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pd.crosstab(child_older_df['Survived'], child_older_df['survived_youngest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27bdf9d8-1bcf-48b3-bdbb-9b0633392085",
    "_uuid": "fac16597328bc4577ba95f81f7193f605537cfe3"
   },
   "source": [
    "These agree nearly all of the time.  Get a data frame with inferred survival statuses!\n",
    "Use the following procedures:\n",
    "* For mothers/nannies, fill in the youngest child's survival status if available\n",
    "* For children, fill in the mother/nanny's survival if available\n",
    "* For older children not yet inferred, fill in the youngest child's survival, if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "36c9ec38-4974-4436-a2e3-cb986572b70d",
    "_uuid": "cbd53891bc924a9bf1fdd0e815c10c30b88aa435"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mother_nanny_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-40b6dab3021e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmother_nanny_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mother_nanny_flag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mchild_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mother_nanny_flag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_analysis_infer_ticket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmother_nanny_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_analysis_infer_ticket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mother_nanny_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mother_nanny_df['mother_nanny_flag'] = 1\n",
    "child_df['mother_nanny_flag'] = 0\n",
    "df_analysis_infer_ticket = pd.concat([mother_nanny_df, child_df], axis=0)\n",
    "\n",
    "df_analysis_infer_ticket.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "39a978ef-daea-490a-823a-68b272f6594a",
    "_uuid": "469c28701a2fa50aafe959610319069eec614008",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_inferred_survival(data_row):\n",
    "    \"\"\"Takes a row from the ticket inferrence frame and infers a survival status.\n",
    "    Returns the survival status as a Series\"\"\"\n",
    "    if ((data_row['mother_nanny_flag'] == 0) &\n",
    "        (not(np.isnan(data_row['survived_mother_nanny'])))):\n",
    "        return (data_row['survived_mother_nanny'])\n",
    "    else:\n",
    "        return (data_row['survived_youngest'])                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "c71daa69-9fab-4322-a145-9ee89d33341a",
    "_uuid": "94a782268bea57e191a515f2c2a4c0a6672f4b71",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis_infer_ticket' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ca512315aa66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df_analysis_infer_ticket['survived_ticket_inferred'] = (df_analysis_infer_ticket\n\u001b[0m\u001b[0;32m      2\u001b[0m                                                         .apply(lambda x: set_inferred_survival(x),\n\u001b[0;32m      3\u001b[0m                                                        axis=1))  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis_infer_ticket' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df_analysis_infer_ticket['survived_ticket_inferred'] = (df_analysis_infer_ticket\n",
    "                                                        .apply(lambda x: set_inferred_survival(x),\n",
    "                                                       axis=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "5b202337-3e30-4e43-ab54-34607d09537e",
    "_uuid": "7ebc7c681c7eb7762f37b3f1e909ebff8dcc7ef9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis_infer_ticket' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f3fc027f403b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_analysis_infer_ticket\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Survived'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis_infer_ticket\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'survived_ticket_inferred'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis_infer_ticket' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pd.crosstab(df_analysis_infer_ticket['Survived'], df_analysis_infer_ticket['survived_ticket_inferred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "2d47fc89-3ad9-4d19-8255-0b2a461d8ba0",
    "_uuid": "c3db6c1a357912228adaadbed4d51f413bad85d3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis_infer_ticket' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-508b02ba64b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m len(df_analysis_infer_ticket[np.isnan(df_analysis_infer_ticket['Survived']) &\n\u001b[0m\u001b[0;32m      6\u001b[0m    ~np.isnan(df_analysis_infer_ticket['survived_ticket_inferred'])])\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis_infer_ticket' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# For how many rows can this procedure infer survival?\n",
    "#\n",
    "\n",
    "len(df_analysis_infer_ticket[np.isnan(df_analysis_infer_ticket['Survived']) &\n",
    "   ~np.isnan(df_analysis_infer_ticket['survived_ticket_inferred'])])\n",
    "\n",
    "# 38 isn't a lot of cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "44a6442c-60b2-4e26-ba94-3aed72eb64a9",
    "_uuid": "0f1a4fbfd04f0eead8b491fc97be927807c1615b",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-66d984dd17cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m df_analysis = pd.merge(df_analysis, df_analysis_infer_ticket[['survived_ticket_inferred', 'mother_nanny_flag']],\n\u001b[0m\u001b[0;32m      6\u001b[0m                       how = 'left', left_index = True, right_index=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Add in the inferred survival, and the mother/nanny flag\n",
    "#\n",
    "\n",
    "df_analysis = pd.merge(df_analysis, df_analysis_infer_ticket[['survived_ticket_inferred', 'mother_nanny_flag']],\n",
    "                      how = 'left', left_index = True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "42e87c4b-ab72-4c9a-89c2-437a33f9f68c",
    "_uuid": "004ce1738e2a4ee4d76299f1c640f30fbd228f5f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-ecdcab50520d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mother_nanny_flag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mother_nanny_flag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df_analysis['mother_nanny_flag'] = df_analysis['mother_nanny_flag'].fillna(0)\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f5f01fb-34fe-48ce-ba3f-871b20e2962c",
    "_uuid": "1e59937966ea85c9a7cb0aa3554da08f184f5c4b"
   },
   "source": [
    "## 3. Gathering Analysis Fields\n",
    "\n",
    "Here, I write a function to process a data frame, modifying and adding fields as needed.  I also create binary fields for categorical variables.  Many steps in processing were performed above, but I consolidate them here so that they can easily be applied to both the training and test data sets.  Functions defined above are used in this process.\n",
    "\n",
    "Major steps are:\n",
    "1.  Calculate family size\n",
    "2.  Get the title \"group\" field\n",
    "3.  Get the modified deck string\n",
    "4.  Calculate the ticket share\n",
    "5.  Fill in missing values in embarkation point\n",
    "6.  Fill in missing values for ages\n",
    "7.  Create numeric (binary) fields for categorical variables\n",
    "\n",
    "The analysis_df has many of these fields already filled and so will be used to create the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af43c00b-acc8-422e-b535-68c1cba7ff5b",
    "_uuid": "409399fb205da17197b296fa18730b00360746a8"
   },
   "source": [
    "### 3A.  Binary encoding of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "1e65f7e3-470c-49ab-a655-68d54a0e1a39",
    "_uuid": "8cd5fc7ca1163354205b59293be70b690a662ba9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get functions for binary encoding of categorical variables in our data sets.\n",
    "# Assume we pass lists of possible values\n",
    "#\n",
    "\n",
    "def binary_encode_dict(value_list):\n",
    "    \"\"\"Gets a dictionary for mapping values to a list of binary digits\n",
    "    for binary encoding of categorical variables\"\"\"\n",
    "    \n",
    "    # Get the # of items in the list and the binary digits required to encode\n",
    "    max_bin = len(value_list)\n",
    "    num_bits = math.ceil(math.log(max_bin, 2))\n",
    "\n",
    "    \n",
    "    # Get the binary encodings for each level\n",
    "    str_dict = {value_list[i]: list(\"{0:b}\".format(i).zfill(num_bits))\n",
    "                for i in range(0, max_bin)}\n",
    "    \n",
    "    # Return the bit count and encoding dict\n",
    "    return (num_bits, str_dict)\n",
    "\n",
    "def binary_encode_series(name_list, data_series, col_prefix):\n",
    "    \"\"\"Apply binary encoding to values in a series, returning a data\n",
    "    frame consisting of the encoded values, with sequential columns\"\"\"\n",
    "    \n",
    "    (col_num, col_dict) = binary_encode_dict(name_list)\n",
    "    col_df = pd.DataFrame(data_series.apply(lambda x: col_dict[x])\n",
    "                          .values.tolist(),\n",
    "                          columns=[col_prefix + str(i) for i in range(0,col_num)],\n",
    "                         index = data_series.index)\n",
    "    return(col_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9de4fafb-e92f-418e-8bd3-8d2a443a765b",
    "_uuid": "0def02bc63abb5c77d4180cc6c4409e468ebded0"
   },
   "source": [
    "### 3B.  Function to process and clean the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "5da2406b-1016-4ea9-95c9-8b8072c4a8c9",
    "_uuid": "7deeab80f82b03c47cb9e0c925dabc51419cc3d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Write the function that processes the data set, cleaning up fields,\n",
    "# encoding categorial fields, etc.\n",
    "#\n",
    "\n",
    "def df_prepare_analysis_binary(input_df, analysis_df):\n",
    "    \"\"\"Get a data frame for analysis using the random forest model.\n",
    "    Use binary encoding of categorical variables.  Assume some\n",
    "    predictors have been filled in via the analysis_df object\"\"\"\n",
    "    \n",
    "    # Get the pre-processed fields for the data of interest\n",
    "    this_analysis_data = analysis_df.loc[input_df.index]\n",
    "    \n",
    "    # Add the ticket share\n",
    "    ret_df = this_analysis_data[['SibSp', 'Parch', 'Pclass', 'ticket_share', 'deck_mod', \n",
    "                                'title_grp', 'family_size', 'age_mod',\n",
    "                                'Sex', 'embarked_mod', 'Fare', 'survived_ticket_inferred',\n",
    "                                'mother_nanny_flag']]\n",
    "\n",
    "    # Fill in any NA fares\n",
    "    fare_mod_ser = ret_df['Fare'].fillna(np.median(analysis_df['Fare'].dropna()))\n",
    "    fare_mod_ser.name = 'fare_mod'\n",
    "\n",
    "    # Binary encoding for categorical fields - Sex\n",
    "    sex_ser_names = ['male', 'female']\n",
    "    sex_df = binary_encode_series(sex_ser_names, ret_df['Sex'], 'sex_')\n",
    "    \n",
    "    # Binary encoding for categorical fields - Deck (modified)\n",
    "    deck_ser_names = ['A', 'B', 'C', 'D', 'E', 'F', 'TG', 'unk']\n",
    "    deck_df = binary_encode_series(deck_ser_names, ret_df['deck_mod'], 'deck_mod_')\n",
    "\n",
    "    # Binary encoding for categorical fields - Title (modified)\n",
    "    title_grp_names = ['mr', 'mrs', 'miss', 'master', 'other']\n",
    "    title_df = binary_encode_series(title_grp_names, ret_df['title_grp'], 'title_grp_')\n",
    "    \n",
    "    # Binary encoding for categorical fields - Embarkation point (modified)\n",
    "    embarked_ser_names = ['S', 'Q', 'C']\n",
    "    emb_df = binary_encode_series(embarked_ser_names, ret_df['embarked_mod'], 'embarked_mod_')\n",
    "\n",
    "    ret_df = pd.concat([ret_df, fare_mod_ser, emb_df, sex_df, deck_df, title_df], axis=1)\n",
    "    return(ret_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85986321-af72-4b94-bfa3-d8140b179ba0",
    "_uuid": "97fb9a4bad7c32ac79d70cc47417b9fcb3b4cac5"
   },
   "source": [
    "## 4.  ExtraTrees Model - Testing and Optimization\n",
    "\n",
    "Here, I use the training data only to explore some features of the ExtraTrees model.  I will split our training data into smaller training and \"test\" portions.  I will look at feature importances, whether the \"ticket share\" field is really valuable, how changing model parameters (# trees, etc.) affects results, and how much the random state affects the model.  \n",
    "\n",
    "The ExtraTrees model does all splits randomly, and so different random states may lead to different accuracy scores.  Moreover, different training data sets may also change results.  I am not using the official \"test\" data here, but rather assessing performance using the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "6047ab63-447c-48d8-82b6-262395dbb6b5",
    "_uuid": "e9b7d57edef6a279d6138540ee9f7640a2da40e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create a function for testing the Extra Trees model on the data set.\n",
    "# This will allow us to vary parameters for the model, including\n",
    "# the predictor, random state, # of trees, etc.  We will also be\n",
    "# able to assess the effects of different training/test sets\n",
    "#  \n",
    "\n",
    "def test_extra_trees_model(train_test_df, predictor_columns,\n",
    "                          tt_split_size =0.3, tt_random_state = None,\n",
    "                          et_n_estimators=10, et_random_state = None,\n",
    "                          et_max_features = 'auto', et_min_samples_split=2,\n",
    "                          et_max_leaf_nodes = None):\n",
    "    '''Test the ExtraTrees model with the training data and certain predictor columns.\n",
    "    The training data will be split into \"training\" and \"test\" portions.  The random state \n",
    "    of the split may be passed in as a parameter.  After the split, the ExtraTrees model is run\n",
    "    and accuracy results obtained.  Extra trees parameters and random states may be passed in as\n",
    "    parameters also.'''\n",
    "    \n",
    "    # Get the test data - split our training set.\n",
    "    train_test_df_X = train_test_df[predictor_columns]\n",
    "    train_test_df_Y = train_test_df['Survived']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(train_test_df_X, \n",
    "                                                        train_test_df_Y,\n",
    "                                                        test_size=tt_split_size,\n",
    "                                                        random_state = tt_random_state)\n",
    "    \n",
    "    # Fit the model, and get feature importances\n",
    "    model = sk.ensemble.ExtraTreesClassifier(n_estimators = et_n_estimators,\n",
    "                                             random_state = et_random_state,\n",
    "                                             max_features = et_max_features,\n",
    "                                             min_samples_split = et_min_samples_split,\n",
    "                                             max_leaf_nodes = et_max_leaf_nodes)\n",
    "    fitted_model = model.fit(X_train, Y_train)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Process importances into data frame\n",
    "    importances_dict = {predictor_columns[i]: importances[i] for i in range(0, len(importances))}\n",
    "    importances_df = (pd.DataFrame.\n",
    "                  from_dict(importances_dict, orient=\"index\")\n",
    "                  .rename(columns={0: 'importance'})\n",
    "                 .sort_values(by='importance', ascending=False))\n",
    "    \n",
    "    # Get the predictions, adding in information from the mother/child inferrences\n",
    "    predictions_raw = fitted_model.predict(X_test)\n",
    "    predictions_ticket = np.array(train_test_df.loc[X_test.index, 'survived_ticket_inferred'], dtype=pd.Series)\n",
    "    vfunc = np.vectorize(lambda x,y: y if np.isnan(x) else x)\n",
    "    predictions = vfunc(predictions_ticket, predictions_raw)\n",
    "    \n",
    "    # Get the predictions, accuracy score, and confusion matrics\n",
    "    confusion_matrix = sk.metrics.confusion_matrix(Y_test,predictions)\n",
    "    accuracy_score = sk.metrics.accuracy_score(Y_test, predictions)\n",
    "    \n",
    "    return (importances_df, confusion_matrix, accuracy_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ad64a19-0b60-452c-9c48-a400cfcbbd25",
    "_uuid": "00056ec33c76f512c100e849eea98af836f72855"
   },
   "source": [
    "### 4A.  Preliminary ExtraTrees run and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "c8fc04b0-2734-4aa8-8be3-947488b9b618",
    "_uuid": "dfa406ea2a017999d7f2fbf8850b2fbac7995cd5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Defne the predictor columns\n",
    "#\n",
    "\n",
    "predictor_columns = ['Parch', 'fare_mod', 'Pclass', 'family_size', 'ticket_share',\n",
    "                     'age_mod', 'sex_0', \n",
    "                     'deck_mod_0','deck_mod_1', 'deck_mod_2', \n",
    "                     'title_grp_0', 'title_grp_1', 'title_grp_2',\n",
    "                     'embarked_mod_0', 'embarked_mod_1', 'mother_nanny_flag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "3e6e5ec8-b1c6-406a-ad3a-131fb8be892c",
    "_uuid": "28629e42c00b169700b026f2413a2eab8c5207dc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ed2cb58d7ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m train_mod_df = pd.concat([df_prepare_analysis_binary(train_data, df_analysis),\n\u001b[0m\u001b[0;32m      7\u001b[0m                           train_data['Survived']], axis=1)\n\u001b[0;32m      8\u001b[0m (importances_df, confusion_matrix, accuracy_score) = test_extra_trees_model(train_mod_df,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Run a simple ExtraTrees test, with default parameters.\n",
    "# Print the accuracy and confusion matrix\n",
    "#\n",
    "\n",
    "train_mod_df = pd.concat([df_prepare_analysis_binary(train_data, df_analysis),\n",
    "                          train_data['Survived']], axis=1)\n",
    "(importances_df, confusion_matrix, accuracy_score) = test_extra_trees_model(train_mod_df,\n",
    "                                                                           predictor_columns)\n",
    "\n",
    "# print the accuracy info\n",
    "print('accuracy score: {0:.4g}'.format(accuracy_score))\n",
    "print('false positives: {0:d}; sensitivity: {1:.3g}'\n",
    "      .format(confusion_matrix[0,1],\n",
    "              (confusion_matrix[0,0]/sum(confusion_matrix[0,:]))))\n",
    "print('false negatives: {0:d}; specificity: {1:.3g}'\n",
    "      .format(confusion_matrix[1,0],\n",
    "              (confusion_matrix[1,1]/sum(confusion_matrix[1,:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "152fd35b-0d05-4ea8-8126-fd4f4f370094",
    "_uuid": "0c4e04a23f0cf0be874db544ad269cf219c85e09"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importances_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-58c01708068a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print information about feature importances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportances_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'barh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'False'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'importance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Variable importances in ExtraTrees model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'importances_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Print information about feature importances\n",
    "importances_df[::-1].plot(kind='barh', legend='False');\n",
    "plt.xlabel('importance');\n",
    "plt.title('Variable importances in ExtraTrees model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62d6cf34-d603-45c1-a73a-8c782e15b414",
    "_uuid": "4e0c5cb3aadb9675e263e6608e51ecb7feec785b"
   },
   "source": [
    "The new \"ticket share\" field I created has a fairly high importance.  Whether this field truly improves the model will be discussed more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "962e8247-66a0-4e45-a74e-e27e7fe3e3c4",
    "_uuid": "68bb9708abb9591bc7209a54527784e5405c7c5e"
   },
   "source": [
    "### 4B.  Random effects on model predictions - train/test split and random state\n",
    "\n",
    "Test how repeating the fit changes the accuracy score.  First, examine the random state of the sytem, keeping the same cross-validation split for now.  \n",
    "\n",
    "Note that the ExtraTrees model does random splits, and so the random state will affect its decisions strongly.  We might expect that repeted runs will result in different forests with different accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "bd15c003-a4ec-4848-8f0e-0642d2923abc",
    "_uuid": "eb7a69ebb269481d5e524940d5d659c70012f1d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-bedd08494b19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                                         \u001b[0mtt_random_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                         et_random_state = x))[2] for\n\u001b[1;32m---> 12\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-bedd08494b19>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m                                         \u001b[0mtt_random_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                         et_random_state = x))[2] for\n\u001b[1;32m---> 12\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Use dictionary comprehension to repeat the model tests.  Use the\n",
    "# same cross-validation split\n",
    "#\n",
    "\n",
    "num_items_test = 101\n",
    "\n",
    "accuracy_dict_et = {x:(test_extra_trees_model(train_mod_df,\n",
    "                                        predictor_columns,\n",
    "                                        tt_random_state = 100,\n",
    "                                        et_random_state = x))[2] for\n",
    "                x in range(0, num_items_test)}\n",
    "\n",
    "\n",
    "accuracy_dict_et_val = list(accuracy_dict_et.values())\n",
    "plt.hist(accuracy_dict_et_val);\n",
    "plt.title('Accuracy scores from various model random states');\n",
    "print('mean accuracy: {}'.format(np.mean(accuracy_dict_et_val)))\n",
    "\n",
    "# Get the index of the median accuracy, for later random state testing\n",
    "median_rs = sorted(accuracy_dict_et, key = accuracy_dict_et.__getitem__)[int(num_items_test/2)]\n",
    "print('median accuracy: {}'.format(accuracy_dict_et[median_rs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1932fc71-65d4-4450-a7f1-01dab0d810a8",
    "_uuid": "0e8af713ceffc2e73a2b6cf263f4615ab79c2aec"
   },
   "source": [
    "It appears there's a pretty wide range of results from this model. Also, a large number of samples are needed to get a good histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "8be6b022-2192-4157-973c-ef0805b79923",
    "_uuid": "da1b719b35d688f85a7c09fa28aa9cdf3dae8ba4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-6d8acd0e8c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                                         \u001b[0mtt_random_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                         et_random_state = median_rs))[2] for\n\u001b[1;32m---> 14\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-6d8acd0e8c51>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m                                         \u001b[0mtt_random_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                         et_random_state = median_rs))[2] for\n\u001b[1;32m---> 14\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Let's see how different training slices affect the score.\n",
    "# Keep the ExtraTrees random state constant for now (use the\n",
    "# median state selected above)\n",
    "# Keep the train/test split constant for now (set a random state)\n",
    "#\n",
    "\n",
    "num_items_test = 101\n",
    "\n",
    "accuracy_dict_tt = {x:(test_extra_trees_model(train_mod_df,\n",
    "                                        predictor_columns,\n",
    "                                        tt_random_state = x,\n",
    "                                        et_random_state = median_rs))[2] for\n",
    "                x in range(0, num_items_test)}\n",
    "\n",
    "\n",
    "accuracy_dict_tt_val = list(accuracy_dict_tt.values())\n",
    "plt.hist(accuracy_dict_tt_val);\n",
    "plt.title('Accuracy scores from various training sets');\n",
    "print('mean accuracy: {}'.format(np.mean(accuracy_dict_et_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "835e6f8b-4fac-41ac-97c4-2241567b6120",
    "_uuid": "75b126f16d2762990ea6dbaced3d2f5acfe9c64a"
   },
   "source": [
    "There is a broad peak.  Characteristics of the training/test sets matter a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff2ffa97-2415-471f-acda-d4113a96a0c7",
    "_uuid": "4977cd08bbd3be7ddc9ead98a3aa4bb697320c19"
   },
   "source": [
    "### 4C.  Evaluation of the \"ticket share\" field.\n",
    "\n",
    "Is the ticket_share variable I created helpful?  Try fits with and without this parameter. \n",
    "\n",
    "The \"ticket share\" field had a fairly high importance (see above).  However, in my (not extensive) experience, highly correlated variables will all tend to have high imporatance, even though adding these fields to the model does not provide additional information nor improve overall accuracy.  Some forests will tend to use one variable, some forests another, leading to votes for both fields.  Therefore, I want to test whether removing the ticket share field from the model affects accuracy.  Given the variability of results, I need to perform multiple tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "b5779127-0c87-45d5-80c3-5e912d57dbc6",
    "_uuid": "8c8da26187fc7f79ba199169a7a846e3f104c26a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-9ef76a256861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m st_pred_items = {x:(test_extra_trees_model(train_mod_df,\n\u001b[0;32m      8\u001b[0m                                         predictor_columns))[2] for\n\u001b[1;32m----> 9\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-9ef76a256861>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m st_pred_items = {x:(test_extra_trees_model(train_mod_df,\n\u001b[0;32m      8\u001b[0m                                         predictor_columns))[2] for\n\u001b[1;32m----> 9\u001b[1;33m                 x in range(0, num_items_test)}\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Do multiple ET fits, letting both the test slice and the ET random state float\n",
    "#\n",
    "\n",
    "num_items_test = 200\n",
    "\n",
    "st_pred_items = {x:(test_extra_trees_model(train_mod_df,\n",
    "                                        predictor_columns))[2] for\n",
    "                x in range(0, num_items_test)}\n",
    "\n",
    "\n",
    "predictor_columns_no_ticket_share = [x for x in predictor_columns\n",
    "                                    if x != 'ticket_share']\n",
    "\n",
    "ns_pred_items = {x:(test_extra_trees_model(train_mod_df,\n",
    "                                        predictor_columns_no_ticket_share))[2] for\n",
    "                x in range(0, num_items_test)}\n",
    "\n",
    "\n",
    "ticket_compare_df = pd.concat([pd.Series(list(st_pred_items.values()), name='With'),\n",
    "          pd.Series(list(ns_pred_items.values()), name='Without')], axis=1)\n",
    "\n",
    "# Create a box plot\n",
    "sns.boxplot(x=['With', 'Without'], y=[ticket_compare_df['With'], ticket_compare_df['Without']]);\n",
    "plt.title('Comparison of model results with and without ticket share');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46a83919-b139-44c3-b52e-ba3757e873e5",
    "_uuid": "e98914bf1d06e7c3003eb6d98e6fce789f7e0792"
   },
   "source": [
    "From the above, it doesn't look like the ticket share variable improves predictions in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b47b37d-7e11-45ad-af17-e30ee0bd62a9",
    "_uuid": "5bb2b77e521859f74de8b368c77d3d5c8953b0b7"
   },
   "source": [
    "### 4D.  Binary vs. one-hot encoding of categorical fields\n",
    "\n",
    "Did binary encoding of categorical fields help the model? \n",
    "\n",
    "Here, I compare binary to \"one hot\" (pd.get_dummies) encoding.  I run repeated predictions to get an idea whether the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "08e3fa57-995b-4826-970b-6dffe30921c3",
    "_uuid": "291155681cc100e8753d22b41b90b4dd644e3449"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-befdedb208d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Add \"one hot\" columns to the data frame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mind_col_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'deck_mod'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Sex'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title_grp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embarked_mod'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m one_hot_df = pd.get_dummies(train_mod_df[ind_col_list], \n\u001b[0m\u001b[0;32m      8\u001b[0m                             \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deck_hot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sex_hot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title_hot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'emb_hot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                             columns=ind_col_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Compare a model with one-hot encoding vs. binary encoding.\n",
    "#\n",
    "\n",
    "# Add \"one hot\" columns to the data frame.\n",
    "ind_col_list = ['deck_mod', 'Sex', 'title_grp', 'embarked_mod']\n",
    "one_hot_df = pd.get_dummies(train_mod_df[ind_col_list], \n",
    "                            prefix=['deck_hot', 'sex_hot', 'title_hot', 'emb_hot'],\n",
    "                            columns=ind_col_list)\n",
    "train_mod_df_hot = pd.concat([train_mod_df, one_hot_df], axis=1)\n",
    "\n",
    "\n",
    "predictor_columns_hot = ([x for x in predictor_columns\n",
    "                                    if x not in \n",
    "                                     ['sex_0', 'deck_mod_0','deck_mod_1', \n",
    "                                      'deck_mod_2', 'title_grp_0', 'title_grp_1', \n",
    "                                      'title_grp_2','embarked_mod_0', 'embarked_mod_1']] +\n",
    "                                     ['sex_hot_female','deck_hot_A','deck_hot_B', \n",
    "                                      'deck_hot_C', 'deck_hot_D','deck_hot_E', 'deck_hot_F', \n",
    "                                      'deck_hot_TG', 'title_hot_master', \n",
    "                                      'title_hot_miss', 'title_hot_mr','title_hot_mrs',\n",
    "                                       'emb_hot_C', 'emb_hot_Q'])\n",
    "\n",
    "\n",
    "train_mod_df_hot[predictor_columns_hot].head()\n",
    "\n",
    "hot_pred_items = {x:(test_extra_trees_model(train_mod_df_hot,\n",
    "                                        predictor_columns_hot))[2] for\n",
    "                x in range(0, num_items_test)}\n",
    "\n",
    "hot_compare_df = pd.concat([pd.Series(list(st_pred_items.values()), name='Binary'),\n",
    "          pd.Series(list(hot_pred_items.values()), name='OneHot')], axis=1)\n",
    "\n",
    "sns.boxplot(x=['Binary', 'OneHot'], y=[hot_compare_df['Binary'], hot_compare_df['OneHot']]);\n",
    "plt.title('Categorical variable encoding effects on model results');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9e483ea6-0f58-45be-b43f-74a563bc8f61",
    "_uuid": "0cf45b521a150cada2b034745128a5cb010e5e53"
   },
   "source": [
    "The results are not very sensitive to encoding.  Binary may be slightly better, and less prone to very low results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "40890db6-d099-41dc-b004-551d2dd16a08",
    "_uuid": "4517acf3f9c9c188d557c9f92e1a8056b5e255e8"
   },
   "source": [
    "### 4E.  Model tuning - tree count\n",
    "\n",
    "Does the number of estimators affect model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "75724ded-df86-4155-b950-67968b720f39",
    "_uuid": "5313da2426eb45a03eccf6a925ef539f1946d94f",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-2f49b6b27fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            et_n_estimators=t)[2] for\n\u001b[1;32m---> 12\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-2f49b6b27fd7>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            et_n_estimators=t)[2] for\n\u001b[1;32m---> 12\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-2f49b6b27fd7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            et_n_estimators=t)[2] for\n\u001b[1;32m---> 12\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Tune the number of trees.  Do multiple trials per tree count, and \n",
    "# try a tree count from 1 to 30\n",
    "#\n",
    "\n",
    "tree_count = range(1, 30)\n",
    "num_trials = 20\n",
    "\n",
    "st_pred_items = {t: [test_extra_trees_model(train_mod_df,\n",
    "                                           predictor_columns,\n",
    "                                           et_n_estimators=t)[2] for\n",
    "                x in range(0,num_trials)] for t in tree_count}\n",
    "\n",
    "tree_count_df = pd.DataFrame.from_dict(st_pred_items, orient='index')\n",
    "\n",
    "test_df = pd.concat([tree_count_df.reset_index()['index'], \n",
    "                     tree_count_df.mean(axis=1), \n",
    "                     tree_count_df.sem(axis=1)], \n",
    "                    axis=1).dropna()\n",
    "test_df.columns = ['trees', 'mean', 'sem']\n",
    "\n",
    "test_df.plot.scatter(x='trees', y='mean', yerr='sem');\n",
    "plt.title('Tree count variation in model results');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60d9f1db-13de-4854-b250-81bed99eaf10",
    "_uuid": "877b279bc72bd9fb7b2addf896f024895b79cc5a"
   },
   "source": [
    "It appears that the optimal tree count plateaus at around 5-10 or so (the default).  We could try a slightly larger # of trees or leave at the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8db97c9-41a9-4e3a-b991-6503e087dcd7",
    "_uuid": "a13514efd1bc2d6f9af41eae5f101d0d58194ea0"
   },
   "source": [
    "### 4F.  Model tuning- number of samples used in splits\n",
    "\n",
    "Does the number of samples required for node splits affect model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "5963154b-741d-44c4-8ad5-5cd900418f27",
    "_uuid": "7344c1e4a0603191dbd9e20b28c516ed185c8c35"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-1f53b3db9b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            et_min_samples_split=s)[2] for\n\u001b[1;32m---> 11\u001b[1;33m                 x in range(0,num_trials)] for s in split_count}\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msplit_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-1f53b3db9b18>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            et_min_samples_split=s)[2] for\n\u001b[1;32m---> 11\u001b[1;33m                 x in range(0,num_trials)] for s in split_count}\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msplit_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-1f53b3db9b18>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m                                            \u001b[0mpredictor_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                            et_min_samples_split=s)[2] for\n\u001b[1;32m---> 11\u001b[1;33m                 x in range(0,num_trials)] for s in split_count}\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msplit_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Tune the # of samples required for node splits\n",
    "#\n",
    "\n",
    "split_count = range(2, 25)\n",
    "num_trials = 20\n",
    "\n",
    "sp_pred_items = {s: [test_extra_trees_model(train_mod_df,\n",
    "                                           predictor_columns,\n",
    "                                           et_min_samples_split=s)[2] for\n",
    "                x in range(0,num_trials)] for s in split_count}\n",
    "\n",
    "split_count_df = pd.DataFrame.from_dict(sp_pred_items, orient='index')\n",
    "\n",
    "test_df = pd.concat([split_count_df.reset_index()['index'], \n",
    "                     split_count_df.mean(axis=1), \n",
    "                     split_count_df.sem(axis=1)], \n",
    "                    axis=1).dropna()\n",
    "test_df.columns = ['split samps', 'mean', 'sem']\n",
    "\n",
    "test_df.plot.scatter(x='split samps', y='mean', yerr='sem');\n",
    "plt.title('Split sample count effects on model results');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c5e7df9-723d-42c4-8788-4212931d655a",
    "_uuid": "dd3c8e0327fd99fc0b9b3a5afaa3f1bdf48af8e2"
   },
   "source": [
    "More samples (>~10) seem to do better than the default of 2. We may want to increase this parameter to be 5 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dda06809-3d48-47db-a820-221142b9b873",
    "_uuid": "22b2a7e2b1f7f8c0f2176226d7348f30e66d7b3b"
   },
   "source": [
    "A larger # of samples used for splits makes trees smaller, in effect simplifying models.  This will reduce variance and increase bias.  In previous explorations, it seemed that fits are pretty sensitive to training/test splits; therefore, reducing variance further may be desirable.  \n",
    "\n",
    "There doesn't seem to be a point in the above chart where bias becomes too great.  However, choosing a smaller value above the shoulder is probably best.  For this data, a split count of 10 may be good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70f6885f-eefd-49c1-9428-a0ac81fa1015",
    "_uuid": "a58392104f81baaeacbc7811e17596a5bc130c10"
   },
   "source": [
    "### 4G.  Model tuning - max feature counts\n",
    "\n",
    "The max feature count determines the degree of randomization in the model.  This is the # of predictors examined when deciding how to split a particular node.  When max_features = 1, the attribute to be split is selected entirely at random.  At the other extreme, all attributes could be screend (this defeats the purpose of the extra trees algorithm to some extent, as increased randomization is a feature of this model). \n",
    "\n",
    "The default value of et_max_features is sqrt(n_features), or sqrt(14) ~ 3 or 4 in this model.  Another default set point available to the model is log2(n_features), which would be ~4 also.  The Python implementation of Extra Trees allows tuning this parameter to any integer value, which I attempt below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "c409aba5-a98b-44e3-ad95-3f4e450b11e6",
    "_uuid": "68e4adddbed67b4eb6efc0772d83e06b504a8d28"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-83ea81ca3ac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                            \u001b[0met_max_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                            et_min_samples_split=10)[2] for\n\u001b[1;32m---> 17\u001b[1;33m                 x in range(0,num_trials)] for f in feature_count}\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mfeature_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-83ea81ca3ac9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m                                            \u001b[0met_max_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                            et_min_samples_split=10)[2] for\n\u001b[1;32m---> 17\u001b[1;33m                 x in range(0,num_trials)] for f in feature_count}\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mfeature_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-83ea81ca3ac9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m                                            \u001b[0met_max_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                            et_min_samples_split=10)[2] for\n\u001b[1;32m---> 17\u001b[1;33m                 x in range(0,num_trials)] for f in feature_count}\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mfeature_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Try different # of features considered at the splits.\n",
    "# The default is sqrt(n_features).\n",
    "#\n",
    "\n",
    "#\n",
    "# Tune the # of features required for node splits\n",
    "#\n",
    "\n",
    "feature_count = range(1, len(predictor_columns))\n",
    "num_trials = 20\n",
    "\n",
    "fp_pred_items = {f: [test_extra_trees_model(train_mod_df,\n",
    "                                           predictor_columns,\n",
    "                                           et_max_features=f,\n",
    "                                           et_min_samples_split=10)[2] for\n",
    "                x in range(0,num_trials)] for f in feature_count}\n",
    "\n",
    "feature_count_df = pd.DataFrame.from_dict(fp_pred_items, orient='index')\n",
    "\n",
    "test_df = (pd.concat([feature_count_df.mean(axis=1), \n",
    "                     feature_count_df.sem(axis=1)], \n",
    "                    axis=1).reset_index()\n",
    "           .dropna())\n",
    "test_df.columns = ['features', 'mean', 'sem']\n",
    "\n",
    "test_df.plot.scatter(x='features', y='mean', yerr='sem');\n",
    "plt.gcf().suptitle('Model tuning: # features compared to split')\n",
    "plt.title('Model effects for # features compared at splits');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b22642b7-0659-4b50-b0cb-bbdf9a034032",
    "_uuid": "b62cc275a851cb528e39133d1072aed4056fe786"
   },
   "source": [
    "The model seems pretty insensitive to the number of features, except at the very low end.  Therefore, I will leave this parameter alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b06da16c-8837-4a52-af3d-7772b706b6d7",
    "_uuid": "ee10ddfa914acdea735ca87a48168bbbe5ea7c4a"
   },
   "source": [
    "### 4H. Model tuning - max number of leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "037438f1-6b20-4cf4-bac5-81ab20565991",
    "_uuid": "c7e8321cbe131bd76ef39d7ae7c458b9ba82a8c4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-b4c4b3d01c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes = t)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in nodes}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnode_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-b4c4b3d01c89>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes = t)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in nodes}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnode_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-b4c4b3d01c89>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes = t)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in nodes}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mnode_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Try to tune the # of leaf nodes\n",
    "#\n",
    "\n",
    "nodes = range(5, 200, 10)\n",
    "num_trials = 20\n",
    "\n",
    "st_pred_items = {t: [test_extra_trees_model(train_mod_df,\n",
    "                                           predictor_columns,\n",
    "                                           et_n_estimators=10,\n",
    "                                           et_min_samples_split=15,\n",
    "                                           et_max_leaf_nodes = t)[2] for\n",
    "                x in range(0,num_trials)] for t in nodes}\n",
    "\n",
    "node_count_df = pd.DataFrame.from_dict(st_pred_items, orient='index')\n",
    "\n",
    "test_df = (pd.concat([node_count_df.mean(axis=1), \n",
    "                     node_count_df.sem(axis=1)], \n",
    "                    axis=1).reset_index()\n",
    "           .dropna())\n",
    "test_df.columns = ['leaf_nodes', 'mean', 'sem']\n",
    "\n",
    "test_df.plot.scatter(x='leaf_nodes', y='mean', yerr='sem');\n",
    "plt.title('Leaf node count effects on model results - higher split count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aa0f01cd-3b13-47e8-9cca-951a6ed5a6b6",
    "_uuid": "a606d76e59c75cb9ba69a07be64532599c0f3461"
   },
   "source": [
    "It appears that increasing the leaf nodes past 20 or so is not helpful in the model.  I want to keep trees simpler and will choose 25 as the max leaf count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f58b0390-a488-4510-afc4-c870527fc60a",
    "_uuid": "0d07ff289926a5915b883ac2b295f6d7c1b50c82"
   },
   "source": [
    "### 4I. Model tuning - tree count with larger split samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "84b579e7-eba9-4cb0-89cb-1def3ed87103",
    "_uuid": "7c52ac73cdb51082bc008ab74215c899ac1df9e1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-1402109766c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes=25)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-1402109766c1>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes=25)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-1402109766c1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0met_min_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            et_max_leaf_nodes=25)[2] for\n\u001b[1;32m---> 13\u001b[1;33m                 x in range(0,num_trials)] for t in tree_count}\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtree_count_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_pred_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Try the tree-tune again, but with better split samps\n",
    "#\n",
    "\n",
    "tree_count = range(1, 100, 5)\n",
    "num_trials = 20\n",
    "\n",
    "st_pred_items = {t: [test_extra_trees_model(train_mod_df,\n",
    "                                           predictor_columns,\n",
    "                                           et_n_estimators=t,\n",
    "                                           et_min_samples_split=15,\n",
    "                                           et_max_leaf_nodes=25)[2] for\n",
    "                x in range(0,num_trials)] for t in tree_count}\n",
    "\n",
    "tree_count_df = pd.DataFrame.from_dict(st_pred_items, orient='index')\n",
    "\n",
    "test_df = (pd.concat([tree_count_df.mean(axis=1), \n",
    "                     tree_count_df.sem(axis=1)], \n",
    "                    axis=1).reset_index()\n",
    "           .dropna())\n",
    "test_df.columns = ['trees', 'mean', 'sem']\n",
    "\n",
    "test_df.plot.scatter(x='trees', y='mean', yerr='sem');\n",
    "plt.title('Tree count effects on model results - higher split count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "442e2f4c-d114-4f7a-9805-a469a2896aea",
    "_uuid": "bc0e37a21278699306ccf54fc87cdd8ec088e927"
   },
   "source": [
    "It is possible that more trees (~20 or so) may be better here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a1a9261-1506-4066-b311-fad0cdf850a3",
    "_uuid": "5f50149d8b8ab8779d20a82d23cc88b9b8fb2b67"
   },
   "source": [
    "## 5.  Predictions and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cab46683-8acc-49c1-a964-5200d96d734a",
    "_uuid": "6bda9e99f4307d9ea66483480e9fb7d3914e89af"
   },
   "source": [
    "### 5A.  Function - predictions from tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "b29ce97c-019e-4da8-be41-68a734df4600",
    "_uuid": "e16ee6ac1059d2ccee38299afe7b72eef68cff2f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Write a function to return predictions for a test set, based\n",
    "# on a training set\n",
    "#\n",
    "\n",
    "def test_extra_trees_model_prediction(train_df, test_df,\n",
    "                                      predictor_columns,\n",
    "                                      get_accuracy = False,\n",
    "                                      et_n_estimators=20, \n",
    "                                      et_random_state = None,\n",
    "                                      et_max_features = 'auto', \n",
    "                                      et_min_samples_split=15,\n",
    "                                      et_max_leaf_nodes=25):\n",
    "    '''Train an ExtraTrees model, and get a prediction.  Optionlly \n",
    "    test the prediction (via an accuracy score).  '''\n",
    "\n",
    "    # Get the training data\n",
    "    X_train = train_df[predictor_columns]\n",
    "    Y_train = train_df['Survived']\n",
    "    \n",
    "    # Fit the model\n",
    "    model = sk.ensemble.ExtraTreesClassifier(n_estimators = et_n_estimators,\n",
    "                                             random_state = et_random_state,\n",
    "                                             max_features = et_max_features,\n",
    "                                             min_samples_split = et_min_samples_split,\n",
    "                                             max_leaf_nodes = et_max_leaf_nodes)\n",
    "    fitted_model = model.fit(X_train, Y_train)\n",
    "\n",
    "    # Get the predictions,\n",
    "    X_test = test_df[predictor_columns]\n",
    "    predictions_raw = fitted_model.predict(X_test)\n",
    "    predictions_ticket = np.array(test_df.loc[X_test.index, 'survived_ticket_inferred'], dtype=pd.Series)\n",
    "    vfunc = np.vectorize(lambda x,y: y if np.isnan(x) else x)\n",
    "    predictions = vfunc(predictions_ticket, predictions_raw)\n",
    "    pred_ser = pd.Series(predictions, index = X_test.index).sort_index()\n",
    "\n",
    "    # Get an accuracy score if indicated\n",
    "    if (get_accuracy):\n",
    "        Y_test = test_df['Survived']\n",
    "        accuracy_score = sk.metrics.accuracy_score(Y_test, predictions)\n",
    "    else:\n",
    "        accuracy_score = None\n",
    "    \n",
    "    return (pred_ser, accuracy_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "035c317b-a689-4846-acac-4b90224a1a90",
    "_uuid": "047ff7f3f826c59eb1c181b54fd8fb03838fb267"
   },
   "source": [
    "### 5B.  Ensembling - repeating models and combining results\n",
    "\n",
    "As the random state affects model outcomes, it may be possible to game the Kaggle scoring by trying different submissions.  In addition, repeated runs of this kernel could result in significantly different scores.  I will look at repeating runs and using a majority vote for final predictions.\n",
    "\n",
    "Before creating the final predictions, I test how ensembling affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "5940abd9-f5cf-4025-a2b9-5f103e1e1b45",
    "_uuid": "e34ef8d9e42dd1b4924151f9f5e04306617c7822"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-974c91b1be4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mens_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mens_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mod_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Test ensembling by splitting the training data again.\n",
    "# Check that this ensembling brings the overall score near\n",
    "# the average score for individual runs\n",
    "#\n",
    "\n",
    "ens_train, ens_test = train_test_split(train_mod_df, test_size=0.2, random_state = 1)\n",
    "\n",
    "#\n",
    "# Repeat modeling for a range of random states\n",
    "#\n",
    "\n",
    "num_trials = 21\n",
    "\n",
    "st_pred_items = [test_extra_trees_model_prediction(ens_train, ens_test, predictor_columns,\n",
    "                                                    get_accuracy=True,\n",
    "                                                    et_min_samples_split=15,\n",
    "                                                    et_random_state = x) \n",
    "                 for x in range(0,num_trials)]\n",
    "\n",
    "predictions = [item[0] for item in st_pred_items]\n",
    "scores = [item[1] for item in st_pred_items]\n",
    "\n",
    "#\n",
    "# Combine predictions - use weighted \n",
    "# average majority vote for each observation\n",
    "#\n",
    "\n",
    "p_df = pd.DataFrame(predictions).transpose().sort_index()\n",
    "ens_pred = p_df.sum(axis=1).apply(lambda x: 1 if x > int(num_trials/2) else 0)\n",
    "\n",
    "ens_Y_test = ens_test['Survived'].sort_index()\n",
    "accuracy_score = sk.metrics.accuracy_score(ens_Y_test, ens_pred)\n",
    "print('ensemble accuracy score:{}'.format(accuracy_score))\n",
    "print('mean score for individual predictions: {}'.format(np.mean(scores)))\n",
    "print('std dev for individual predictions: {}'.format(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f9d97dc-6707-4184-b935-868db19b4ec9",
    "_uuid": "b081520945dc4482f9c31f61ee11011958294b5a"
   },
   "source": [
    "### 5C.  Final training and submission data\n",
    "\n",
    "The final predictions are based on a majority vote of all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "2083a636-72ae-438a-982b-ad88b71ffec6",
    "_uuid": "6fe0d5ea5632e97c0b80e2ef3ce139ade5551744"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-0fd5694cabc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest_mod_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_prepare_analysis_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Re-train the model using all the available training data.  \n",
    "# Then apply the model to the test data, including ensembling\n",
    "#\n",
    "\n",
    "test_mod_df = df_prepare_analysis_binary(test_data, df_analysis)\n",
    "\n",
    "\n",
    "#\n",
    "# Repeat modeling for a range of random states\n",
    "#\n",
    "\n",
    "num_trials = 21\n",
    "\n",
    "fin_pred_items = [test_extra_trees_model_prediction(train_mod_df, test_mod_df, \n",
    "                                                   predictor_columns,\n",
    "                                                   get_accuracy=False,\n",
    "                                                   et_min_samples_split=15,\n",
    "                                                   et_random_state = x) \n",
    "                 for x in range(0,num_trials)]\n",
    "\n",
    "predictions = [item[0] for item in fin_pred_items]\n",
    "\n",
    "#\n",
    "# Combine predictions - use weighted \n",
    "# average majority vote for each observation\n",
    "#\n",
    "\n",
    "final_pred_df = pd.DataFrame(predictions).transpose().sort_index()\n",
    "ens_pred = (final_pred_df.sum(axis=1)\n",
    "            .apply(lambda x: 1 if x > int(num_trials/2) else 0))\n",
    "ens_pred.name = 'Survived'\n",
    "ens_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "28ac99bd-394f-4ee3-ad6d-8eec726459a2",
    "_uuid": "25fb8857c998df68fad833fe28d63cc2434560a2",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ens_pred' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fc372a671f52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mens_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predictions_vc20171003.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ens_pred' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Export the predictions to a file\n",
    "#\n",
    "\n",
    "ens_pred.to_csv('predictions_vc20171003.csv', header='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ff2ee7f-b75f-4100-97bc-9746e098f069",
    "_uuid": "6831ac888e452ec1223622445cea4b56c4036a73"
   },
   "source": [
    "Do some very simple sanity checks on the predictions.  Does the overall survival rate make sense?  What about rates by sex and age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "16dac43d-3269-4398-b26f-2814d7b34f4b",
    "_uuid": "0d2d8c6cecbca16443ea344609d0f7c651efd191"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-c91a0a3c7906>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m print('training data survival rate: {0:.3g}'\n\u001b[1;32m----> 2\u001b[1;33m       .format(train_mod_df['Survived'].mean()))\n\u001b[0m\u001b[0;32m      3\u001b[0m print('test predicted survival rate: {0:.3g}'\n\u001b[0;32m      4\u001b[0m       .format(ens_pred.mean()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print('training data survival rate: {0:.3g}'\n",
    "      .format(train_mod_df['Survived'].mean()))\n",
    "print('test predicted survival rate: {0:.3g}'\n",
    "      .format(ens_pred.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "a54ff0c2-99b8-41fa-964e-4b72770eeb8e",
    "_uuid": "919856bf2705a65990c2b14749d32c563ac28e09"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e201a7dc620f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msex_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'train'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain_mod_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtest_mod_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msex_comp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msex_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msex_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Survival by sex\n",
    "#\n",
    "\n",
    "sex_dict = {'train' : train_mod_df[['Sex']], 'test' : test_mod_df[['Sex']]}\n",
    "sex_comp_df = pd.concat(sex_dict.values(),axis=0,keys=sex_dict.keys())\n",
    "\n",
    "sur_dict = {'train' : train_mod_df['Survived'], 'test' : ens_pred}\n",
    "sur_comp_df = pd.DataFrame(pd.concat(sur_dict.values(),axis=0,keys=sur_dict.keys()))\n",
    "\n",
    "test_df = pd.concat([sex_comp_df, sur_comp_df], axis=1)\n",
    "\n",
    "t = (test_df.reset_index(0)\n",
    "     .pivot_table(index='Sex', columns='level_0', aggfunc=[np.mean, scipy.stats.sem]))\n",
    "t['mean'].plot(kind='bar', yerr=t['sem']);\n",
    "plt.title('Survival rates by sex: test and training data');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "81f25e8a-1c9b-4869-9a1b-2e3629a60f40",
    "_uuid": "6b9d5e897d36b791f5a2a3a566140ea123d4eefc",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-e00dace7a0a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m age_dict = {'train' : train_mod_df['age_mod'], \n\u001b[0m\u001b[0;32m      6\u001b[0m             'test' : test_mod_df['age_mod']}\n\u001b[0;32m      7\u001b[0m \u001b[0mage_comp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mage_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#\n",
    "# Survival by age.\n",
    "#\n",
    "\n",
    "age_dict = {'train' : train_mod_df['age_mod'], \n",
    "            'test' : test_mod_df['age_mod']}\n",
    "age_comp_df = pd.concat(age_dict.values(),axis=0,keys=age_dict.keys())\n",
    "\n",
    "test_df = pd.concat([age_comp_df, sur_comp_df], axis=1)\n",
    "test_df['age_grp'] = test_df['age_mod'].apply(lambda x: 20*int(x/20)if x < 60 else 60)\n",
    "\n",
    "t = (test_df.reset_index(0)\n",
    "     .pivot_table(index='age_grp', values='Survived',\n",
    "                  columns='level_0', aggfunc=[np.mean, scipy.stats.sem]))\n",
    "t.head()\n",
    "t['mean'].plot(kind='bar', yerr=t['sem']);\n",
    "plt.title('Survival rates by age grp: test and training data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "b830c547-0b6a-4c17-b5c8-b7ed03de24f2",
    "_uuid": "d2805e52c3f733dbc980428d384f4067cb631a31"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_mod_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-96d683003a99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_mod_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'age_mod'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_mod_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "ax = sns.distplot(test_mod_df['age_mod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0409077a-0a2d-40f8-98f6-c5e5529dd9fd",
    "_uuid": "ca4cf4b671ad972c70c765a44098e486c80877a0"
   },
   "source": [
    "The test predictions and training data seem to have some differences in survival rates by sex and age.  Presumably interaction terms are a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e309b05b-fb7d-4adf-adbe-4bb3e7d89b83",
    "_uuid": "508c9d2ccfd378a1b4c2f415e9e9e14f61439448"
   },
   "source": [
    "## 6.  Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "305eeea5-2f38-4a8c-8748-179a30bd6381",
    "_uuid": "2d0f6bb2140639571907424cf34910ceecefedbd"
   },
   "source": [
    "This is my third Extra Trees trial.  Here, I have added some assumptions about mothers/nannies and children who share a ticket.\n",
    "\n",
    "Data maniplulations:  Binary encoding of categorical variables reduced the degrees of freedom, and may have improved predictions,  but also reduce interpretability.  I believe that interpretability is important in generalizing to other data sources.  The ticket_share field did not seem to improve model accuracy significantly.\n",
    "\n",
    "Model tuning: Increasing the number of samples considered at a split seemed to improve predictions.  Ensembling reduced variability due to the random state of the system.\n",
    "\n",
    "Mother/nanny/child assumptions:  Using ticket-matched survival resulted in a slight increase in my score. \n",
    "\n",
    "Usefulness:  Maching learning is relevant when you want to guess an unknown outcome, not when you want to understand factors that contribute to an effect.  There is usually a reason you want the predictons (an intervention).  Interventions I can imagine might be selling insurance to passengers or ships, or perhaps predicting whether you should get on a ship.  The predictions here are based on many variables that are very specific to the titanic, and/or are not actionable.  The characteristics of the ship itself may be more important in predicting survival, compared to passenger characteristics.  I might want a simpler model for predictions for other ships/voyages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b65f8b5e-e424-450e-a8f3-e5f11f992a0c",
    "_uuid": "6756271abee93b274ae89a01ce652560231c57cd",
    "collapsed": true
   },
   "source": [
    "## 7.  Referfences\n",
    "\n",
    "Extremely Randomized Trees:  http://montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf\n",
    "\n",
    "Ensembling: https://mlwave.com/kaggle-ensembling-guide/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
